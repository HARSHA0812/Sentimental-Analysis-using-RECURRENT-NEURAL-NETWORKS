{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\SAI BALAJI OFFICE\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 544, 8)            80000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 544, 16)           1200      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 544, 8)            600       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 81,961\n",
      "Trainable params: 81,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('output.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.spatial.distance import cdist\n",
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import download\n",
    "import imdb\n",
    "imdb.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-set size:  25000\n",
      "Test-set size:   25000\n"
     ]
    }
   ],
   "source": [
    "x_train_text, y_train = imdb.load_data(train=True) #loading train data\n",
    "x_test_text, y_test = imdb.load_data(train=False) # loading test data\n",
    "print(\"Train-set size: \", len(x_train_text))\n",
    "print(\"Test-set size:  \", len(x_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221.27714\n",
      "2209\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "data_text = x_train_text + x_test_text\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data_text)\n",
    "tokenizer.word_index\n",
    "\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test_text) # converting text data into tokens\n",
    "#print(x_train_text[2])\n",
    "#print(np.array(x_train_tokens[0]))\n",
    "num_tokens = np.array([len(tokens) for tokens in x_train_tokens + x_test_tokens])\n",
    "print(np.mean(num_tokens))\n",
    "print(np.max(num_tokens))\n",
    "print(np.min(num_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n",
      "94.53 %\n"
     ]
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)\n",
    "print(str(np.sum(num_tokens < max_tokens) / len(num_tokens) * 100) +' %' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 220s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "pad = 'pre'\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "num_tokens_pad = np.array([len(tokens) for tokens in x_train_pad + x_test_pad])\n",
    "result = model.evaluate(x_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86208284] [0.59047353]\n",
      "Positive Review with a score of 86.2082839012146 %\n",
      "Negative Review with a score of 59.04735326766968 %\n"
     ]
    }
   ],
   "source": [
    "positive_review='''its very good movie'''\n",
    "\n",
    "negative_review='''its very bad movie'''\n",
    "\n",
    "text=[positive_review,negative_review]\n",
    "tokens = tokenizer.texts_to_sequences(text) # we need to tokenize\n",
    "\n",
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating='pre')\n",
    "# padding\n",
    "a=model.predict(tokens_pad)[0]\n",
    "b=model.predict(tokens_pad)[1]\n",
    "print(a,b)\n",
    "\n",
    "if a > 0.60: # I am thresholding it at 60%\n",
    "    print('Positive Review with a score of {} %'.format(a[0]*100))\n",
    "else:\n",
    "    print('Negative Review ')\n",
    "\n",
    "\n",
    "if b < 0.50: # I am thresholding it at 50%\n",
    "    print('Positive Review ')\n",
    "else:\n",
    "    print('Negative Review with a score of {} %'.format(b[0]*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
